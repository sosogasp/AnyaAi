{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc49cfa",
   "metadata": {},
   "source": [
    "# Agente com Gemini + System Prompt via `prefix`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea53a3a",
   "metadata": {},
   "source": [
    "Este notebook mostra como usar o modelo Gemini com `initialize_agent`, adicionando um system prompt corretamente via `agent_kwargs['prefix']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90397e",
   "metadata": {},
   "source": [
    "## FLUXOGRAMA\n",
    "                          +---------------------+\n",
    "                          | Início do Processo  |\n",
    "                          +----------+----------+\n",
    "                                     |\n",
    "                                     v\n",
    "                     +---------------+----------------+\n",
    "                     | Pergunta do aluno é definida   |\n",
    "                     +---------------+----------------+\n",
    "                                     |\n",
    "                                     v\n",
    "         +--------------------------+---------------------------+\n",
    "         | Agente LangChain (tutor) responde usando Gemini      |\n",
    "         | - Usa memória de conversa                            |\n",
    "         | - Segue prompt de tutor                              |\n",
    "         +--------------------------+---------------------------+\n",
    "                                     |\n",
    "                                     v\n",
    "                +--------------------+--------------------+\n",
    "                | Resposta do tutor armazenada e exibida  |\n",
    "                +--------------------+--------------------+\n",
    "                                     |\n",
    "                                     v\n",
    "                   +----------------+----------------+\n",
    "                   | Juiz (novo modelo Gemini)       |\n",
    "                   | - Temperatura mais baixa        |\n",
    "                   | - Segue prompt crítico          |\n",
    "                   +----------------+----------------+\n",
    "                                     |\n",
    "                                     v\n",
    "          +--------------------------+---------------------------+\n",
    "          | Função avaliar_resposta é chamada                    |\n",
    "          | - Junta pergunta e resposta do tutor                 |\n",
    "          | - Envia para o juiz avaliar com base nos critérios   |\n",
    "          +--------------------------+---------------------------+\n",
    "                                     |\n",
    "                                     v\n",
    "                       +---------------------------+\n",
    "                       | Juiz retorna avaliação:   |\n",
    "                       |  Aprovado ou  Reprovado   |\n",
    "                       +---------------------------+\n",
    "                                     |\n",
    "                                     v\n",
    "                        +-----------------------+\n",
    "                        | Exibe resultado final |\n",
    "                        +-----------+-----------+\n",
    "                                    |\n",
    "                                    v\n",
    "                            +-------+------+\n",
    "                            | Fim do Fluxo |\n",
    "                            +--------------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee6285",
   "metadata": {},
   "source": [
    "## **Instalação das bibliotecas**\n",
    "* `langchain`: orquestrador de fluxos com LLMs (Large Language Models).\n",
    "* `langchain-google-genai`: integração com o modelo **Gemini** (Google).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73a7b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instale as bibliotecas necessárias se ainda não tiver\n",
    "# !pip install langchain langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e21052",
   "metadata": {},
   "source": [
    "## **Imports e ambiente**\n",
    "* `from langchain_google_genai import ChatGoogleGenerativeAI`\n",
    "\n",
    " \tEste comando importa a classe que conecta o LangChain ao modelo Gemini do Google.\n",
    "    Você usa essa classe para criar um objeto de conversa com IA (como um chatbot ou tutor), definindo o modelo (gemini-2.0, por exemplo), temperatura, e chave de API.\n",
    "* `from langchain.memory import ConversationBufferMemory`\n",
    "\n",
    "\tImporta a classe responsável por guardar o histórico das conversas entre o usuário e o modelo, permitindo que o agente “lembre” do que foi dito antes.\n",
    "\tA memória é do tipo buffer, ou seja, linear e simples, sem lógica de recuperação semântica.\n",
    "* `from langchain.agents import initialize_agent, AgentType`\n",
    "\n",
    " \tinitialize_agent: função que cria um agente LangChain completo, juntando LLM, memória e ferramentas.\n",
    " \tAgentType: enumeração que define o tipo de raciocínio que o agente vai usar.\n",
    "    Exemplo: CHAT_CONVERSATIONAL_REACT_DESCRIPTION cria um agente que age por etapas e lembra do histórico.\n",
    "* `import google.generativeai as genai`\n",
    "\n",
    "    Este é o pacote oficial do Google para trabalhar com o Gemini diretamente, sem LangChain.\n",
    "\tAqui, você usa genai.configure() para informar a chave da API.\n",
    "\tMesmo que LangChain use sua própria integração, o código precisa configurar a chave com esta biblioteca.\n",
    "* `import os`\n",
    "\n",
    " \tBiblioteca padrão do Python para acessar variáveis do sistema operacional.\n",
    " \tUsado aqui para pegar a chave da API guardada em uma variável de ambiente: os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "* `from dotenv import load_dotenv`\n",
    "\n",
    "    Permite carregar variáveis de ambiente definidas em um arquivo .env, que geralmente fica na raiz do projeto.\n",
    "\n",
    "    Exemplo do arquivo .env:\n",
    "    GEMINI_API_KEY=chave_super_secreta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5197678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91673dfa",
   "metadata": {},
   "source": [
    "## **Configuração do Gemini**\n",
    "\n",
    "* `api_key`: busca a chave da API do Gemini do `.env`.\n",
    "* `temperature=0.7`: controla a **criatividade** da resposta (0 = conservador, 1 = criativo).\n",
    "* `model=\"gemini-2.0-flash\"`: versão rápida do modelo do Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7009a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    google_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573ba27",
   "metadata": {},
   "source": [
    "## **Prompt do sistema**\n",
    "Esse é o **manual do personagem** da IA: o “tutor”. Ele será usado para definir o comportamento inicial do agente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cf22c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = '''\n",
    "Você é um tutor de Inteligência Artificial amigável e experiente.\n",
    "Explique conceitos de forma clara e concisa, adapte-se ao nível do aluno\n",
    "e sempre ofereça o próximo passo lógico no aprendizado.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee93f5",
   "metadata": {},
   "source": [
    "## ** Memória da conversa**\n",
    "Essa célula cria a \"memória de curto prazo\" da IA. Ela armazena as mensagens trocadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c102733",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27cd10",
   "metadata": {},
   "source": [
    "## **Inicialização do agente**\n",
    "Aqui você junta tudo:\n",
    "* `llm=chat`: o modelo Gemini.\n",
    "* `memory`: memória da conversa.\n",
    "* `AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION`: tipo de agente que **pensa em etapas**.\n",
    "* `verbose=True`: imprime os passos internos (útil para debugging).\n",
    "* `prefix`: texto do tutor (prompt do sistema).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00f65d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    llm=chat,\n",
    "    tools=[],\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    agent_kwargs={\n",
    "        \"prefix\": system_prompt_text\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722466d6",
   "metadata": {},
   "source": [
    "## **Pergunta do aluno**\n",
    "Você envia uma **pergunta de aluno**, e o `agent` responde com base no comportamento de tutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b54ae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"LangChain e LangGraph são ambos frameworks para construir aplicações com modelos de linguagem, mas eles abordam o problema de maneiras diferentes. LangChain é um framework mais geral, oferecendo uma ampla gama de ferramentas e componentes para construir diferentes tipos de aplicações LLM, incluindo cadeias (chains), agentes e recuperação de informação. LangGraph, por outro lado, é projetado especificamente para construir aplicações LLM que requerem fluxos de trabalho mais complexos e com estado (stateful). Ele permite definir um grafo de passos, onde cada passo pode ser um modelo de linguagem, uma ferramenta ou outra função. LangGraph é mais adequado para aplicações que precisam rastrear o estado ao longo do tempo e tomar decisões com base nesse estado, como agentes conversacionais complexos ou fluxos de trabalho de decisão automatizados. Em resumo, LangChain é mais versátil e oferece uma ampla gama de recursos, enquanto LangGraph é mais especializado para aplicações com fluxos de trabalho complexos e com estado.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "LangChain e LangGraph são ambos frameworks para construir aplicações com modelos de linguagem, mas eles abordam o problema de maneiras diferentes. LangChain é um framework mais geral, oferecendo uma ampla gama de ferramentas e componentes para construir diferentes tipos de aplicações LLM, incluindo cadeias (chains), agentes e recuperação de informação. LangGraph, por outro lado, é projetado especificamente para construir aplicações LLM que requerem fluxos de trabalho mais complexos e com estado (stateful). Ele permite definir um grafo de passos, onde cada passo pode ser um modelo de linguagem, uma ferramenta ou outra função. LangGraph é mais adequado para aplicações que precisam rastrear o estado ao longo do tempo e tomar decisões com base nesse estado, como agentes conversacionais complexos ou fluxos de trabalho de decisão automatizados. Em resumo, LangChain é mais versátil e oferece uma ampla gama de recursos, enquanto LangGraph é mais especializado para aplicações com fluxos de trabalho complexos e com estado.\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"Qual a diferença de langchain para langgraph?\"\n",
    "resposta = agent.run(pergunta)\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44cc05",
   "metadata": {},
   "source": [
    "## **Criando o juiz**\n",
    "Um **segundo modelo** é criado, com `temperature=0.3`, ou seja, **menos criativo**, mais crítico e analítico — ideal para atuar como juiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4525d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "juiz = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.3,\n",
    "    google_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a51ec9",
   "metadata": {},
   "source": [
    "## **Prompt do juiz**\n",
    "Esse texto é o **manual do juiz**. Ele explica os critérios de avaliação e a estrutura da resposta que ele deve dar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca509769",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_juiz = '''\n",
    "Você é um avaliador imparcial. Sua tarefa é revisar a resposta de um tutor de IA para uma pergunta de aluno.\n",
    "\n",
    "Critérios:\n",
    "- A resposta está tecnicamente correta?\n",
    "- Está clara para o nível médio técnico?\n",
    "- O próximo passo sugerido está bem formulado?\n",
    "\n",
    "Se a resposta for boa, diga “✅ Aprovado” e explique por quê.\n",
    "Se tiver problemas, diga “⚠️ Reprovado” e proponha uma versão melhorada.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738976c9",
   "metadata": {},
   "source": [
    "## **Função de avaliação**\n",
    "* `SystemMessage`: define o comportamento do juiz.\n",
    "* `HumanMessage`: insere o contexto da pergunta e da resposta.\n",
    "* `juiz.invoke(...)`: faz o juiz **avaliar a resposta** gerada pelo tutor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c6d8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "def avaliar_resposta(pergunta, resposta_tutor):\n",
    "    mensagens = [\n",
    "        SystemMessage(content=prompt_juiz),\n",
    "        HumanMessage(content=f\"Pergunta do aluno: {pergunta}\\n\\nResposta do tutor: {resposta_tutor}\")\n",
    "    ]\n",
    "    return juiz.invoke(mensagens).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680ac14",
   "metadata": {},
   "source": [
    "## **Rodando a avaliação**\n",
    "Você chama a função de avaliação passando a pergunta e a resposta do tutor, e imprime a análise feita pelo juiz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb189301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliação do juiz:\n",
      " ✅ Aprovado.\n",
      "\n",
      "A resposta está correta, clara e concisa. Ela explica bem a diferença entre Langchain e LangGraph, e indica quando usar cada um.\n"
     ]
    }
   ],
   "source": [
    "avaliacao = avaliar_resposta(pergunta, resposta)\n",
    "print(\"Avaliação do juiz:\\n\", avaliacao)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
