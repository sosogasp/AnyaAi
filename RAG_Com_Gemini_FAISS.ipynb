{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2e9ea5",
   "metadata": {},
   "source": [
    "# üîç RAG com Gemini e FAISS ‚Äì Recupera√ß√£o de Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787f267",
   "metadata": {},
   "source": [
    "Este notebook implementa um pipeline RAG (Retrieval-Augmented Generation) usando Gemini como modelo LLM e FAISS como mecanismo de busca vetorial local.\n",
    "\n",
    "Documentos `.txt` ser√£o carregados de uma pasta, transformados em embeddings, indexados e consultados com base nas perguntas do usu√°rio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e373347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instale os pacotes necess√°rios (se ainda n√£o tiver)\n",
    "# !pip install langchain langchain-google-genai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a83e4",
   "metadata": {},
   "source": [
    "## 1. Carregar e dividir os documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49131cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 documentos carregados.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Define a pasta com os arquivos .txt\n",
    "PASTA_DOCS = \"meus_arquivos\"\n",
    "\n",
    "# Carrega todos os documentos .txt da pasta\n",
    "def carregar_documentos(pasta):\n",
    "    docs = []\n",
    "    for nome in os.listdir(pasta):\n",
    "        if nome.endswith(\".txt\"):\n",
    "            caminho = os.path.join(pasta, nome)\n",
    "            loader = TextLoader(caminho, encoding=\"utf-8\")\n",
    "            docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "documentos = carregar_documentos(PASTA_DOCS)\n",
    "print(f\"{len(documentos)} documentos carregados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db594f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fragmentos gerados.\n"
     ]
    }
   ],
   "source": [
    "# Dividir documentos em peda√ßos menores\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs_divididos = splitter.split_documents(documentos)\n",
    "print(f\"{len(docs_divididos)} fragmentos gerados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991cd80",
   "metadata": {},
   "source": [
    "## 2. Gerar embeddings e criar √≠ndice FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d4eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Carregar embeddings do Gemini\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    google_api_key=api_key,\n",
    "    model=\"models/embedding-001\"\n",
    "    )\n",
    "\n",
    "# Criar vetor FAISS\n",
    "db = FAISS.from_documents(docs_divididos, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a077a9",
   "metadata": {},
   "source": [
    "## 3. Inicializar o modelo Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edcb285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.5,\n",
    "    google_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b4c11",
   "metadata": {},
   "source": [
    "## 4. Construir o Chain de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae383dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Cria o chain de pergunta-resposta com recupera√ß√£o\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b956f32",
   "metadata": {},
   "source": [
    "## 5. Fazer perguntas ao sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d8af5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:\n",
      " O documento diz que a lua √© de queijo.\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"O que o documento diz sobre a lua?\"\n",
    "resposta = rag_chain(pergunta)\n",
    "print(\"Resposta:\\n\", resposta['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23312a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos usados como fonte:\n",
      "- meus_arquivos\\fakes_1.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Documentos usados como fonte:\")\n",
    "for doc in resposta['source_documents']:\n",
    "    print(\"-\", doc.metadata['source'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
